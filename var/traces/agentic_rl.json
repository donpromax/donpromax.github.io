{
  "files": {
    "agentic_rl.py": "from execute_util import link, image, text\nfrom arxiv_util import arxiv_reference\nfrom lecture_util import article_link, x_link, youtube_link\nfrom reference import join\nfrom references import previous_share_on_rl, agentic_rl_landscape, qwen_tool_call, rstar_agent\n\n\ndef main():\n    welcome()\n    what_is_this_program()\n    # The Landscape of Agentic Reinforcement Learning for LLMs: A Survey\n    difference_between_llm_rl_and_agentic_rl()\n    capability_perspective()\n    vertical_domains()\n    # A tool call example based on Qwen\n    link(title=\"A Tool Demo Based on Qwen3\", url=\"https://donpromax.github.io/trace-viewer/?trace=var%2Ftraces%2Ftool_call_example.json\")\n    \n    rstar2_agent()\n    # rStar2-Agent demo\n    link(title=\"rStar2 Agent Demo\", url=\"https://donpromax.github.io/trace-viewer/?trace=var%2Ftraces%2Frstar_agent.json\")\n\n\ndef welcome():\n    text(\"## Sharing Insights on Training and Inference in Agentic Reinforcement Learning\")\n    text(\"- Previous Sharing on Reinforcement Learning\"), link(previous_share_on_rl)\n    image(\"images/agentic/rl.png\", width=600)\n    text(\"### Overview of today's topic\")\n    text(\"- The Landscape of Agentic Reinforcement Learning for LLMs: A Survey\"), link(agentic_rl_landscape)\n    text(\"- A tool call example based on Qwen&nbsp;\"), link(qwen_tool_call)\n    text(\"- rStar2-Agent: Agentic Reasoning Technical Report\"), link(rstar_agent)\n\n\ndef what_is_this_program():\n    text(\"This is an *executable lecture*, a program whose execution delivers the content of a lecture.\")\n    text(\"Executable lectures make it possible to:\")\n    text(\"- view and run code (since everything is code!),\")\n    total = 0  # @inspect total\n    for x in [1, 2, 3]:  # @inspect x\n        total += x  # @inspect total\n\n\ndef difference_between_llm_rl_and_agentic_rl():\n    text(\"## From LLM RL to Agentic RL\")\n    image(\"images/agentic/llm-rl-to-agentic.jpg\", width=800, center=True)\n    text(\"- Reward design\")\n    text(\"- Transition\")\n    text(\"- Action space\")\n    text(\"- Objective\")\n    text(\"- RL Algorithms\")\n    text(\"- Environments\")\n    \n    text(\"## Markov Decision Processes\")\n    text(\"PBRFT. The RL training process of preference-based Reinforcement fine-tuning (PBRFT) is formalized as a degenerate MDP defined by the tuple:\")\n    text(r\"$$\\langle S_{\\text{trad}}, A_{\\text{trad}}, P_{\\text{trad}}, R_{\\text{trad}}, T = 1 \\rangle$$\", rendering_type=\"mathjax\")\n    text(\"Agentic RL. The RL training process of agentic RL is modeled as a partially observable Markov decision process (POMDP):\")\n    text(r\"$$\\left\\langle S_{\\text{agent}}, A_{\\text{agent}}, P_{\\text{agent}}, R_{\\text{agent}}, \\gamma, O \\right\\rangle$$\", rendering_type=\"mathjax\")\n    image(\"images/agentic/comp_pbrft_and_agenticrl.jpg\", width=800, center=True)\n    \n    text(\"## Training Process\")\n    text(\"For PBRFT:\")\n    text(\"Text as input -> LLM generates multiple outputs -> human feedback -> Using PPO or DPO to RFT the LLM\")\n    text(\"For Agentic RL:\")\n    text(\"Observations as input -> Agent choose actions -> Environment returns new observations and rewards -> policy updates\")\n    image(\"images/agentic/dpo.jpg\", width=400, center=True)\n    image(\"images/agentic/react_rl.jpg\", width=400, center=True)\n    \n    text(\"## Environment\")\n    image(\"images/agentic/agentic_environment.jpg\", width=800, center=True)\n    text(\"An example of TextWorld environment:\"), link(arxiv_reference(\"https://arxiv.org/pdf/2010.03768\"))\n    image(\"images/agentic/alfworld.png\", width=800, center=True)\n    \n    text(\"## Action Space\")\n    text(\"VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use\"), link(arxiv_reference(\"https://arxiv.org/abs/2509.01055v1\"))\n    image(\"images/agentic/verltool.jpg\", width=800, center=True)\n    \n    text(\"## Transition Dynamics\")\n    image(\"images/agentic/transition_dynamics.jpg\", width=800, center=True)\n    \n    text(\"## Learning Objective\")\n    image(\"images/agentic/learning_objective.jpg\", width=800, center=True)\n    \n    text(\"## RL Algorithms\")\n    text(\"- REINFORCE&nbsp;\"), link(title=\"the Policy Gradient Theorem\", url=\"https://huggingface.co/learn/deep-rl-course/unit4/pg-theorem\")\n\n    text(r\"$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$$\", rendering_type=\"mathjax\")\n    text(r\"$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\left( \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right) \\left( \\sum_{t=0}^{T} r(s_t, a_t) \\right) \\right]$$\", rendering_type=\"mathjax\")\n    \n    text(\"- Proximal Policy Optimization (PPO)\"), link(arxiv_reference(\"https://arxiv.org/abs/1707.06347\"))\n    \n    text(r\"$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$$\", rendering_type=\"mathjax\")\n    text(r\"$$L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right]$$\", rendering_type=\"mathjax\")\n    \n    text(\"- Direct Preference Optimization (DPO)\"), link(arxiv_reference(\"https://arxiv.org/abs/2305.18290\"))\n    \n    text(r\"$$L_{DPO}(\\pi_\\theta; \\pi_{ref}) = - \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right) \\right]$$\", rendering_type=\"mathjax\")\n    \n    text(\"- Group Relative Policy Optimization (GRPO)\"), link(arxiv_reference(\"https://arxiv.org/abs/2402.03300\"))\n    \n    text(r\"$$r_{i,t}(\\theta) = \\frac{\\pi_{\\theta}(o_{i,t} \\mid q, o_{i, \\lt t})}{\\pi_{\\theta_{\\mathrm{old}}}(o_{i,t} \\mid q, o_{i, \\lt t})}$$\", rendering_type=\"mathjax\")\n    text(r\"$$L_{i,t}(\\theta) = \\min \\left( r_{i,t}(\\theta) \\hat{A}_{i,t}, \\operatorname{clip}(r_{i,t}(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_{i,t} \\right)$$\", rendering_type=\"mathjax\")\n    text(r\"$$J_{\\text{GRPO}}(\\theta) = \\mathbb{E}_{q, \\{o_i\\}} \\left[ \\frac{1}{G} \\sum_{i=1}^{G} \\sum_{t=0}^{|o_i|-1} L_{i,t}(\\theta) \\right]$$\", rendering_type=\"mathjax\")\n    \n    image(\"images/agentic/variants_of_rl.jpg\", width=800, center=True)\n    \n\ndef capability_perspective():\n    image(\"images/agentic/model_capability.jpg\", width=800, center=True)\n    text(\"LLM Agent = LLM + Reasoning + Planning + Memory + Perception + Tool Use + Self-Improve\")\n    text(\"---\")\n    \n    text(\"## Capabilities\")\n    image(\"images/agentic/agentic_aspects.jpg\", width=800, center=True)\n    \n    rl_for_reasoning_perception()\n    rl_for_planning()\n    rl_for_tool_use()\n    rl_for_agent_memory()\n    rl_for_self_improvement()\n\n\ndef rl_for_reasoning_perception():\n    text(\"### Reasoning\")\n    text(\"- OpenAI's o3 \"), link(\"https://openai.com/index/openai-o3-mini/\")\n    text(\"- DeepSeek's r1\"), link(arxiv_reference(\"https://arxiv.org/pdf/2501.12948.pdf\"))\n    text(\"### Perception\")\n    text(\"**image**\")\n    text(\"- DeepEyes\"), link(arxiv_reference(\"https://arxiv.org/abs/2505.14362\"))\n    text(\"**video**\")\n    text(\"- Video-R1\"), link(arxiv_reference(\"https://arxiv.org/abs/2503.21776\"))\n    text(\"**audio**\")\n    text(\"- EchoInk-R1\"), link(arxiv_reference(\"https://arxiv.org/abs/2505.04623\"))\n    \n\ndef rl_for_planning():\n    text(\"### RL as external driver\")\n    text(\"Reasoning with Language Model is Planning with World Model\"), link(arxiv_reference(\"https://arxiv.org/abs/2305.14992\"))\n    image(\"images/agentic/rl_as_external_guide.jpg\", width=800, center=True)\n    link(title=\"MCT Demo\", url=\"https://github.com/maitrix-org/llm-reasoners/blob/main/demo.ipynb\")\n    text(\"### RL as internal driver\")\n    text(\"Encouraging Good Processes Without the Need for Good Answers\"), link(arxiv_reference(\"https://arxiv.org/abs/2508.19598\", organization=\"Tencent\"))\n    image(\"images/agentic/rl_as_internal_guide.jpg\", width=800, center=True)\n    text(r\"$$R_{total} = \\begin{cases}-1, & \\text{if trajectory format is invalid}, \\\\ R_{comp} + R_{rule}, & \\text{otherwise}. \\end{cases}$$\", rendering_type=\"mathjax\")\n    text(\"where $R_{comp}$ is the tooluse completeness reward, $R_{rule}$ is the rule based reward including a negative repetition reward Rrepeat to discourage redundant tool calls, and a negative reward Rerror as a penalty for incorrect tool usage. \")\n    \n\ndef rl_for_tool_use():\n    image(\"images/agentic/agent_tool_use.jpg\", width=800, center=True)\n    \n    text(\"### ReAct-style Tool Calling\")\n    text(\"Leverages 1)Prompt Engineering or 2)SFT-based methods to enable LLMs to follow the 'Observation-Thinking-Action' tool-use behaviors.\")\n    text(\"AgentBank\"), link(arxiv_reference(\"https://arxiv.org/abs/2410.07706\"))\n    image(\"images/agentic/agent_bank.jpg\", width=800, center=True)\n    \n    text(\"### Tool-integrated RL\")\n    text(\"Enables agents to strategically decide when, how, and in what combination to invoke tools\")\n    text(\"OpenAI o3 o4 DeepResearch&nbsp;\"), link(title=\"Introducing deep research\", url=\"https://openai.com/index/introducing-deep-research/\")\n    text(\"UI-TARS-2\"), link(arxiv_reference(\"https://arxiv.org/abs/2509.02544\"))\n    link(title=\"UI-TARS-2 DEMO\", url=\"https://seed-tars.com/showcase/ui-tars-2/\")\n    image(\"images/agentic/UI-TARS-2.jpg\", width=800, center=True)\n    \n    text(\"### Long-horizon RL\")\n    text(\"Current RL approaches often depend on outcome-based rewards, making it difficult to pinpoint which specific tool invocation in a long, interdependent sequence contributed to success or failure.\")\n    text(\"step-level advantage estimation in SpaRL\"), link(arxiv_reference(\"https://arxiv.org/abs/2505.20732\"))\n    image(\"images/agentic/SpaRL.jpg\", width=800, center=True)\n    \n    \ndef rl_for_agent_memory():\n    image(\"images/agentic/rl_for_memory_overview.jpg\", width=800, center=True)\n    text(\"### RAG-style\")\n    text(\"MemoryBank\"), link(arxiv_reference(\"https://arxiv.org/abs/2305.10250\"))\n    image(\"images/agentic/memory_bank.jpg\", width=800, center=True)\n    text(\"Memory-R1\"), link(arxiv_reference(\"https://arxiv.org/abs/2508.19828\"))\n    image(\"images/agentic/memory-r1.jpg\", width=800, center=True)\n    image(\"images/agentic/overview-of-memory-r1.jpg\", width=800, center=True)\n    \n    text(\"### RL for Token-level Memory\")\n    text(\"MemAgent\"), link(arxiv_reference(\"https://arxiv.org/abs/2507.02259\", organization=\"ByteDance\"))\n    image(\"images/agentic/mem_agent.jpg\", width=800, center=True)\n    text(\"ReSum\"), link(arxiv_reference(\"https://arxiv.org/abs/2509.13313\", organization=\"Alibaba\"))\n    image(\"images/agentic/resum.jpg\", width=800, center=True)\n    text(\"**example for qwen deepresearch:**\")\n    image(\"images/agentic/qwen_deep_research.jpg\", width=800, center=True)\n    link(title=\"\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08Agentic RL\uff09\u6df1\u5ea6\u7814\u7a76\u62a5\u544a.pdf\", url=\"var/files/\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08Agentic RL\uff09\u6df1\u5ea6\u7814\u7a76\u62a5\u544a.pdf\")\n    \n\ndef rl_for_self_improvement():\n    text(\"- RL for Verbal Self-correction.\")\n    text(\"- RL for Internalizing Self-correction.\")\n    text(\"- RL for Iterative Self-training.\")\n    text(\"---\")\n    text(\"### RL for Verbal Self-correction\")\n    text(\"Utilizing prompt engineering/rl base methods.\")\n    text(\"Agents generate an answer, linguistically reflect on its potential errors, and subsequently produce a refined solution.\")\n    text(\"### RL for Internalizing Self-correction\")\n    text(\"ACC-Collab\"), link(arxiv_reference(\"https://arxiv.org/abs/2411.00053\"))\n    image(\"images/agentic/actor_critic.jpg\", width=800, center=True)\n    text(\"### RL for Iterative Self-training\")\n    text(\"Absolute Zero\"), link(arxiv_reference(\"https://arxiv.org/abs/2505.03335\"))\n    image(\"images/agentic/absolute_zero.jpg\", width=800, center=True)\n    text(\"Learning Different Modes of Reasoning:\")\n    text(\"Deduction(\u6f14\u7ece): predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\")\n    text(\"Abduction(\u6eaf\u56e0): inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\")\n    text(\"Induction(\u5f52\u7eb3): synthesizing a program p from a set of in-out examples, requiring generalization from partial information\")\n    \n    text(\"MAPoRL\"), link(arxiv_reference(\"https://arxiv.org/abs/2502.18439\"))\n    image(\"images/agentic/maporl.jpg\", width=800, center=True)\n    image(\"images/agentic/maporl_demo.jpg\", width=800, center=True)\n    \n\ndef vertical_domains():\n    image(\"images/agentic/vertical_domains.jpg\", width=800, center=True)\n\n\ndef rstar2_agent():\n    text(\"rStar2-Agent: Agentic Reasoning Technical Report\"), link(rstar_agent)\n    text(\"## Overview\")\n    text(\"Based on Qwen3-14B trained with agentic reinforcement learning to achieve frontier-level performance\")\n    image(\"images/agentic/rstar2_bench.png\", width=600, center=True)\n    text(\"### An example\")\n    text(\"Agent can use Python coding tools and reflect on code execution feedback\")\n    image(\"images/agentic/rstar2_demo.png\", width=800, center=True)\n    text(\"full information:\"), link(\"https://yuque.antfin.com/vihe9q/oeo4yp/dfunmnxdwq8gxbyn\")"
  },
  "steps": [
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 8,
          "function_name": "main",
          "code": "def main():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 9,
          "function_name": "main",
          "code": "welcome()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 9,
          "function_name": "main",
          "code": "welcome()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 23,
          "function_name": "welcome",
          "code": "def welcome():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 9,
          "function_name": "main",
          "code": "welcome()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 24,
          "function_name": "welcome",
          "code": "text(\"## Sharing Insights on Training and Inference in Agentic Reinforcement Learning\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Sharing Insights on Training and Inference in Agentic Reinforcement Learning",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 9,
          "function_name": "main",
          "code": "welcome()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 25,
          "function_name": "welcome",
          "code": "text(\"- Previous Sharing on Reinforcement Learning\"), link(previous_share_on_rl)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Previous Sharing on Reinforcement Learning",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "\u5f3a\u5316\u5b66\u4e60\u7b80\u5355\u7efc\u8ff0",
            "authors": [
              "Dong Lyu"
            ],
            "organization": "ata",
            "date": "2025",
            "url": "https://ata.atatech.org/articles/12020396838",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 9,
          "function_name": "main",
          "code": "welcome()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 26,
          "function_name": "welcome",
          "code": "image(\"images/agentic/rl.png\", width=600)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/rl.png",
          "style": {
            "width": 600
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 9,
          "function_name": "main",
          "code": "welcome()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 27,
          "function_name": "welcome",
          "code": "text(\"### Overview of today's topic\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Overview of today's topic",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 9,
          "function_name": "main",
          "code": "welcome()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 28,
          "function_name": "welcome",
          "code": "text(\"- The Landscape of Agentic Reinforcement Learning for LLMs: A Survey\"), link(agentic_rl_landscape)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- The Landscape of Agentic Reinforcement Learning for LLMs: A Survey",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "The Landscape of Agentic Reinforcement Learning for LLMs: A Survey",
            "authors": [
              "Guibin Zhang",
              "Hejia Geng",
              "Xiaohang Yu",
              "Zhenfei Yin",
              "Zaibin Zhang",
              "Zelin Tan",
              "Heng Zhou",
              "Zhongzhi Li",
              "Xiangyuan Xue",
              "Yijiang Li",
              "Yifan Zhou",
              "Yang Chen",
              "Chen Zhang",
              "Yutao Fan",
              "Zihu Wang",
              "Songtao Huang",
              "Yue Liao",
              "Hongru Wang",
              "Mengyue Yang",
              "Heng Ji",
              "Michael Littman",
              "Jun Wang",
              "Shuicheng Yan",
              "Philip Torr",
              "Lei Bai"
            ],
            "organization": null,
            "date": "2025-09-02T17:46:26Z",
            "url": "https://arxiv.org/pdf/2509.02547",
            "description": "The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents.",
            "notes": "Agentic Reinforcement survey"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 9,
          "function_name": "main",
          "code": "welcome()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 29,
          "function_name": "welcome",
          "code": "text(\"- A tool call example based on Qwen&nbsp;\"), link(qwen_tool_call)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- A tool call example based on Qwen&nbsp;",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Function Calling",
            "authors": null,
            "organization": "Alibaba",
            "date": null,
            "url": "https://qwen.readthedocs.io/en/latest/framework/function_call.html",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 9,
          "function_name": "main",
          "code": "welcome()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 30,
          "function_name": "welcome",
          "code": "text(\"- rStar2-Agent: Agentic Reasoning Technical Report\"), link(rstar_agent)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- rStar2-Agent: Agentic Reasoning Technical Report",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "rStar2-Agent: Agentic Reasoning Technical Report",
            "authors": [
              "Ning Shang",
              "Yifei Liu",
              "Yi Zhu",
              "Li Lyna Zhang",
              "Weijiang Xu",
              "Xinyu Guan",
              "Buze Zhang",
              "Bingcheng Dong",
              "Xudong Zhou",
              "Bowen Zhang",
              "Ying Xin",
              "Ziming Miao",
              "Scarlett Li",
              "Fan Yang",
              "Mao Yang"
            ],
            "organization": "Microsoft",
            "date": "2025-08-28T12:45:25Z",
            "url": "https://arxiv.org/pdf/2508.20722",
            "description": "We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar.",
            "notes": "Python tool calling, code interpreter as environment"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 9,
          "function_name": "main",
          "code": "welcome()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 10,
          "function_name": "main",
          "code": "what_is_this_program()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 10,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 33,
          "function_name": "what_is_this_program",
          "code": "def what_is_this_program():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 10,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 34,
          "function_name": "what_is_this_program",
          "code": "text(\"This is an *executable lecture*, a program whose execution delivers the content of a lecture.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This is an *executable lecture*, a program whose execution delivers the content of a lecture.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 10,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 35,
          "function_name": "what_is_this_program",
          "code": "text(\"Executable lectures make it possible to:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Executable lectures make it possible to:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 10,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 36,
          "function_name": "what_is_this_program",
          "code": "text(\"- view and run code (since everything is code!),\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- view and run code (since everything is code!),",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 10,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 37,
          "function_name": "what_is_this_program",
          "code": "total = 0  # @inspect total"
        }
      ],
      "env": {
        "total": 0
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 10,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 38,
          "function_name": "what_is_this_program",
          "code": "for x in [1, 2, 3]:  # @inspect x"
        }
      ],
      "env": {
        "x": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 10,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 39,
          "function_name": "what_is_this_program",
          "code": "total += x  # @inspect total"
        }
      ],
      "env": {
        "total": 1
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 10,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 38,
          "function_name": "what_is_this_program",
          "code": "for x in [1, 2, 3]:  # @inspect x"
        }
      ],
      "env": {
        "x": 2
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 10,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 39,
          "function_name": "what_is_this_program",
          "code": "total += x  # @inspect total"
        }
      ],
      "env": {
        "total": 3
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 10,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 38,
          "function_name": "what_is_this_program",
          "code": "for x in [1, 2, 3]:  # @inspect x"
        }
      ],
      "env": {
        "x": 3
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 10,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 39,
          "function_name": "what_is_this_program",
          "code": "total += x  # @inspect total"
        }
      ],
      "env": {
        "total": 6
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 10,
          "function_name": "main",
          "code": "what_is_this_program()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 38,
          "function_name": "what_is_this_program",
          "code": "for x in [1, 2, 3]:  # @inspect x"
        }
      ],
      "env": {
        "x": 3
      },
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 10,
          "function_name": "main",
          "code": "what_is_this_program()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 42,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "def difference_between_llm_rl_and_agentic_rl():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 43,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"## From LLM RL to Agentic RL\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## From LLM RL to Agentic RL",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 44,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "image(\"images/agentic/llm-rl-to-agentic.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/llm-rl-to-agentic.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 45,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"- Reward design\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Reward design",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 46,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"- Transition\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Transition",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 47,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"- Action space\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Action space",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 48,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"- Objective\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Objective",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 49,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"- RL Algorithms\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- RL Algorithms",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 50,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"- Environments\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Environments",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 52,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"## Markov Decision Processes\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Markov Decision Processes",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 53,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"PBRFT. The RL training process of preference-based Reinforcement fine-tuning (PBRFT) is formalized as a degenerate MDP defined by the tuple:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "PBRFT. The RL training process of preference-based Reinforcement fine-tuning (PBRFT) is formalized as a degenerate MDP defined by the tuple:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 54,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(r\"$$\\langle S_{\\text{trad}}, A_{\\text{trad}}, P_{\\text{trad}}, R_{\\text{trad}}, T = 1 \\rangle$$\", rendering_type=\"mathjax\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "mathjax",
          "data": "$$\\langle S_{\\text{trad}}, A_{\\text{trad}}, P_{\\text{trad}}, R_{\\text{trad}}, T = 1 \\rangle$$",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 55,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"Agentic RL. The RL training process of agentic RL is modeled as a partially observable Markov decision process (POMDP):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Agentic RL. The RL training process of agentic RL is modeled as a partially observable Markov decision process (POMDP):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 56,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(r\"$$\\left\\langle S_{\\text{agent}}, A_{\\text{agent}}, P_{\\text{agent}}, R_{\\text{agent}}, \\gamma, O \\right\\rangle$$\", rendering_type=\"mathjax\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "mathjax",
          "data": "$$\\left\\langle S_{\\text{agent}}, A_{\\text{agent}}, P_{\\text{agent}}, R_{\\text{agent}}, \\gamma, O \\right\\rangle$$",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 57,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "image(\"images/agentic/comp_pbrft_and_agenticrl.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/comp_pbrft_and_agenticrl.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 59,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"## Training Process\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Training Process",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 60,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"For PBRFT:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "For PBRFT:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 61,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"Text as input -> LLM generates multiple outputs -> human feedback -> Using PPO or DPO to RFT the LLM\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Text as input -> LLM generates multiple outputs -> human feedback -> Using PPO or DPO to RFT the LLM",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 62,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"For Agentic RL:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "For Agentic RL:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 63,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"Observations as input -> Agent choose actions -> Environment returns new observations and rewards -> policy updates\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Observations as input -> Agent choose actions -> Environment returns new observations and rewards -> policy updates",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 64,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "image(\"images/agentic/dpo.jpg\", width=400, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/dpo.jpg",
          "style": {
            "width": 400,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 65,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "image(\"images/agentic/react_rl.jpg\", width=400, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/react_rl.jpg",
          "style": {
            "width": 400,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 67,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"## Environment\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Environment",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 68,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "image(\"images/agentic/agentic_environment.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/agentic_environment.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 69,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"An example of TextWorld environment:\"), link(arxiv_reference(\"https://arxiv.org/pdf/2010.03768\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "An example of TextWorld environment:",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
            "authors": [
              "Mohit Shridhar",
              "Xingdi Yuan",
              "Marc-Alexandre C\u00f4t\u00e9",
              "Yonatan Bisk",
              "Adam Trischler",
              "Matthew Hausknecht"
            ],
            "organization": null,
            "date": "2020-10-08T05:13:36Z",
            "url": "https://arxiv.org/pdf/2010.03768",
            "description": "Given a simple request like Put a washed apple in the kitchen fridge, humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text based policies in TextWorld (C\\^ot\\'e et al., 2018) and then execute goals from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, and visual scene understanding).",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 70,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "image(\"images/agentic/alfworld.png\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/alfworld.png",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 72,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"## Action Space\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Action Space",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 73,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use\"), link(arxiv_reference(\"https://arxiv.org/abs/2509.01055v1\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use",
            "authors": [
              "Dongfu Jiang",
              "Yi Lu",
              "Zhuofeng Li",
              "Zhiheng Lyu",
              "Ping Nie",
              "Haozhe Wang",
              "Alex Su",
              "Hui Chen",
              "Kai Zou",
              "Chao Du",
              "Tianyu Pang",
              "Wenhu Chen"
            ],
            "organization": null,
            "date": "2025-09-01T01:45:18Z",
            "url": "https://arxiv.org/abs/2509.01055v1",
            "description": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2$\\times$ speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 74,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "image(\"images/agentic/verltool.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/verltool.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 76,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"## Transition Dynamics\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Transition Dynamics",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 77,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "image(\"images/agentic/transition_dynamics.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/transition_dynamics.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 79,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"## Learning Objective\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Learning Objective",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 80,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "image(\"images/agentic/learning_objective.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/learning_objective.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 82,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"## RL Algorithms\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## RL Algorithms",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 83,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"- REINFORCE&nbsp;\"), link(title=\"the Policy Gradient Theorem\", url=\"https://huggingface.co/learn/deep-rl-course/unit4/pg-theorem\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- REINFORCE&nbsp;",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "the Policy Gradient Theorem",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://huggingface.co/learn/deep-rl-course/unit4/pg-theorem",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 85,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(r\"$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$$\", rendering_type=\"mathjax\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "mathjax",
          "data": "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$$",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 86,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(r\"$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\left( \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right) \\left( \\sum_{t=0}^{T} r(s_t, a_t) \\right) \\right]$$\", rendering_type=\"mathjax\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "mathjax",
          "data": "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\left( \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right) \\left( \\sum_{t=0}^{T} r(s_t, a_t) \\right) \\right]$$",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 88,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"- Proximal Policy Optimization (PPO)\"), link(arxiv_reference(\"https://arxiv.org/abs/1707.06347\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Proximal Policy Optimization (PPO)",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Proximal Policy Optimization Algorithms",
            "authors": [
              "John Schulman",
              "Filip Wolski",
              "Prafulla Dhariwal",
              "Alec Radford",
              "Oleg Klimov"
            ],
            "organization": null,
            "date": "2017-07-20T02:32:33Z",
            "url": "https://arxiv.org/abs/1707.06347",
            "description": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 90,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(r\"$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$$\", rendering_type=\"mathjax\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "mathjax",
          "data": "$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$$",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 91,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(r\"$$L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right]$$\", rendering_type=\"mathjax\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "mathjax",
          "data": "$$L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right]$$",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 93,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"- Direct Preference Optimization (DPO)\"), link(arxiv_reference(\"https://arxiv.org/abs/2305.18290\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Direct Preference Optimization (DPO)",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
            "authors": [
              "Rafael Rafailov",
              "Archit Sharma",
              "Eric Mitchell",
              "Stefano Ermon",
              "Christopher D. Manning",
              "Chelsea Finn"
            ],
            "organization": null,
            "date": "2023-05-29T17:57:46Z",
            "url": "https://arxiv.org/abs/2305.18290",
            "description": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 95,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(r\"$$L_{DPO}(\\pi_\\theta; \\pi_{ref}) = - \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right) \\right]$$\", rendering_type=\"mathjax\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "mathjax",
          "data": "$$L_{DPO}(\\pi_\\theta; \\pi_{ref}) = - \\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right) \\right]$$",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 97,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(\"- Group Relative Policy Optimization (GRPO)\"), link(arxiv_reference(\"https://arxiv.org/abs/2402.03300\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Group Relative Policy Optimization (GRPO)",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
            "authors": [
              "Zhihong Shao",
              "Peiyi Wang",
              "Qihao Zhu",
              "Runxin Xu",
              "Junxiao Song",
              "Xiao Bi",
              "Haowei Zhang",
              "Mingchuan Zhang",
              "Y. K. Li",
              "Y. Wu",
              "Daya Guo"
            ],
            "organization": null,
            "date": "2024-02-05T18:55:32Z",
            "url": "https://arxiv.org/abs/2402.03300",
            "description": "Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 99,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(r\"$$r_{i,t}(\\theta) = \\frac{\\pi_{\\theta}(o_{i,t} \\mid q, o_{i, \\lt t})}{\\pi_{\\theta_{\\mathrm{old}}}(o_{i,t} \\mid q, o_{i, \\lt t})}$$\", rendering_type=\"mathjax\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "mathjax",
          "data": "$$r_{i,t}(\\theta) = \\frac{\\pi_{\\theta}(o_{i,t} \\mid q, o_{i, \\lt t})}{\\pi_{\\theta_{\\mathrm{old}}}(o_{i,t} \\mid q, o_{i, \\lt t})}$$",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 100,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(r\"$$L_{i,t}(\\theta) = \\min \\left( r_{i,t}(\\theta) \\hat{A}_{i,t}, \\operatorname{clip}(r_{i,t}(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_{i,t} \\right)$$\", rendering_type=\"mathjax\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "mathjax",
          "data": "$$L_{i,t}(\\theta) = \\min \\left( r_{i,t}(\\theta) \\hat{A}_{i,t}, \\operatorname{clip}(r_{i,t}(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_{i,t} \\right)$$",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 101,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "text(r\"$$J_{\\text{GRPO}}(\\theta) = \\mathbb{E}_{q, \\{o_i\\}} \\left[ \\frac{1}{G} \\sum_{i=1}^{G} \\sum_{t=0}^{|o_i|-1} L_{i,t}(\\theta) \\right]$$\", rendering_type=\"mathjax\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "mathjax",
          "data": "$$J_{\\text{GRPO}}(\\theta) = \\mathbb{E}_{q, \\{o_i\\}} \\left[ \\frac{1}{G} \\sum_{i=1}^{G} \\sum_{t=0}^{|o_i|-1} L_{i,t}(\\theta) \\right]$$",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 103,
          "function_name": "difference_between_llm_rl_and_agentic_rl",
          "code": "image(\"images/agentic/variants_of_rl.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/variants_of_rl.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 12,
          "function_name": "main",
          "code": "difference_between_llm_rl_and_agentic_rl()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 106,
          "function_name": "capability_perspective",
          "code": "def capability_perspective():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 107,
          "function_name": "capability_perspective",
          "code": "image(\"images/agentic/model_capability.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/model_capability.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 108,
          "function_name": "capability_perspective",
          "code": "text(\"LLM Agent = LLM + Reasoning + Planning + Memory + Perception + Tool Use + Self-Improve\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "LLM Agent = LLM + Reasoning + Planning + Memory + Perception + Tool Use + Self-Improve",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 109,
          "function_name": "capability_perspective",
          "code": "text(\"---\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "---",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 111,
          "function_name": "capability_perspective",
          "code": "text(\"## Capabilities\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Capabilities",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 112,
          "function_name": "capability_perspective",
          "code": "image(\"images/agentic/agentic_aspects.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/agentic_aspects.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 114,
          "function_name": "capability_perspective",
          "code": "rl_for_reasoning_perception()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 114,
          "function_name": "capability_perspective",
          "code": "rl_for_reasoning_perception()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 121,
          "function_name": "rl_for_reasoning_perception",
          "code": "def rl_for_reasoning_perception():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 114,
          "function_name": "capability_perspective",
          "code": "rl_for_reasoning_perception()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 122,
          "function_name": "rl_for_reasoning_perception",
          "code": "text(\"### Reasoning\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Reasoning",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 114,
          "function_name": "capability_perspective",
          "code": "rl_for_reasoning_perception()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 123,
          "function_name": "rl_for_reasoning_perception",
          "code": "text(\"- OpenAI's o3 \"), link(\"https://openai.com/index/openai-o3-mini/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- OpenAI's o3 ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://openai.com/index/openai-o3-mini/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 114,
          "function_name": "capability_perspective",
          "code": "rl_for_reasoning_perception()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 124,
          "function_name": "rl_for_reasoning_perception",
          "code": "text(\"- DeepSeek's r1\"), link(arxiv_reference(\"https://arxiv.org/pdf/2501.12948.pdf\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- DeepSeek's r1",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
            "authors": [
              "DeepSeek-AI",
              "Daya Guo",
              "Dejian Yang",
              "Haowei Zhang",
              "Junxiao Song",
              "Ruoyu Zhang",
              "Runxin Xu",
              "Qihao Zhu",
              "Shirong Ma",
              "Peiyi Wang",
              "Xiao Bi",
              "Xiaokang Zhang",
              "Xingkai Yu",
              "Yu Wu",
              "Z. F. Wu",
              "Zhibin Gou",
              "Zhihong Shao",
              "Zhuoshu Li",
              "Ziyi Gao",
              "Aixin Liu",
              "Bing Xue",
              "Bingxuan Wang",
              "Bochao Wu",
              "Bei Feng",
              "Chengda Lu",
              "Chenggang Zhao",
              "Chengqi Deng",
              "Chenyu Zhang",
              "Chong Ruan",
              "Damai Dai",
              "Deli Chen",
              "Dongjie Ji",
              "Erhang Li",
              "Fangyun Lin",
              "Fucong Dai",
              "Fuli Luo",
              "Guangbo Hao",
              "Guanting Chen",
              "Guowei Li",
              "H. Zhang",
              "Han Bao",
              "Hanwei Xu",
              "Haocheng Wang",
              "Honghui Ding",
              "Huajian Xin",
              "Huazuo Gao",
              "Hui Qu",
              "Hui Li",
              "Jianzhong Guo",
              "Jiashi Li",
              "Jiawei Wang",
              "Jingchang Chen",
              "Jingyang Yuan",
              "Junjie Qiu",
              "Junlong Li",
              "J. L. Cai",
              "Jiaqi Ni",
              "Jian Liang",
              "Jin Chen",
              "Kai Dong",
              "Kai Hu",
              "Kaige Gao",
              "Kang Guan",
              "Kexin Huang",
              "Kuai Yu",
              "Lean Wang",
              "Lecong Zhang",
              "Liang Zhao",
              "Litong Wang",
              "Liyue Zhang",
              "Lei Xu",
              "Leyi Xia",
              "Mingchuan Zhang",
              "Minghua Zhang",
              "Minghui Tang",
              "Meng Li",
              "Miaojun Wang",
              "Mingming Li",
              "Ning Tian",
              "Panpan Huang",
              "Peng Zhang",
              "Qiancheng Wang",
              "Qinyu Chen",
              "Qiushi Du",
              "Ruiqi Ge",
              "Ruisong Zhang",
              "Ruizhe Pan",
              "Runji Wang",
              "R. J. Chen",
              "R. L. Jin",
              "Ruyi Chen",
              "Shanghao Lu",
              "Shangyan Zhou",
              "Shanhuang Chen",
              "Shengfeng Ye",
              "Shiyu Wang",
              "Shuiping Yu",
              "Shunfeng Zhou",
              "Shuting Pan",
              "S. S. Li",
              "Shuang Zhou",
              "Shaoqing Wu",
              "Shengfeng Ye",
              "Tao Yun",
              "Tian Pei",
              "Tianyu Sun",
              "T. Wang",
              "Wangding Zeng",
              "Wanjia Zhao",
              "Wen Liu",
              "Wenfeng Liang",
              "Wenjun Gao",
              "Wenqin Yu",
              "Wentao Zhang",
              "W. L. Xiao",
              "Wei An",
              "Xiaodong Liu",
              "Xiaohan Wang",
              "Xiaokang Chen",
              "Xiaotao Nie",
              "Xin Cheng",
              "Xin Liu",
              "Xin Xie",
              "Xingchao Liu",
              "Xinyu Yang",
              "Xinyuan Li",
              "Xuecheng Su",
              "Xuheng Lin",
              "X. Q. Li",
              "Xiangyue Jin",
              "Xiaojin Shen",
              "Xiaosha Chen",
              "Xiaowen Sun",
              "Xiaoxiang Wang",
              "Xinnan Song",
              "Xinyi Zhou",
              "Xianzu Wang",
              "Xinxia Shan",
              "Y. K. Li",
              "Y. Q. Wang",
              "Y. X. Wei",
              "Yang Zhang",
              "Yanhong Xu",
              "Yao Li",
              "Yao Zhao",
              "Yaofeng Sun",
              "Yaohui Wang",
              "Yi Yu",
              "Yichao Zhang",
              "Yifan Shi",
              "Yiliang Xiong",
              "Ying He",
              "Yishi Piao",
              "Yisong Wang",
              "Yixuan Tan",
              "Yiyang Ma",
              "Yiyuan Liu",
              "Yongqiang Guo",
              "Yuan Ou",
              "Yuduan Wang",
              "Yue Gong",
              "Yuheng Zou",
              "Yujia He",
              "Yunfan Xiong",
              "Yuxiang Luo",
              "Yuxiang You",
              "Yuxuan Liu",
              "Yuyang Zhou",
              "Y. X. Zhu",
              "Yanhong Xu",
              "Yanping Huang",
              "Yaohui Li",
              "Yi Zheng",
              "Yuchen Zhu",
              "Yunxian Ma",
              "Ying Tang",
              "Yukun Zha",
              "Yuting Yan",
              "Z. Z. Ren",
              "Zehui Ren",
              "Zhangli Sha",
              "Zhe Fu",
              "Zhean Xu",
              "Zhenda Xie",
              "Zhengyan Zhang",
              "Zhewen Hao",
              "Zhicheng Ma",
              "Zhigang Yan",
              "Zhiyu Wu",
              "Zihui Gu",
              "Zijia Zhu",
              "Zijun Liu",
              "Zilin Li",
              "Ziwei Xie",
              "Ziyang Song",
              "Zizheng Pan",
              "Zhen Huang",
              "Zhipeng Xu",
              "Zhongyu Zhang",
              "Zhen Zhang"
            ],
            "organization": null,
            "date": "2025-01-22T15:19:35Z",
            "url": "https://arxiv.org/pdf/2501.12948.pdf",
            "description": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 114,
          "function_name": "capability_perspective",
          "code": "rl_for_reasoning_perception()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 125,
          "function_name": "rl_for_reasoning_perception",
          "code": "text(\"### Perception\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Perception",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 114,
          "function_name": "capability_perspective",
          "code": "rl_for_reasoning_perception()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 126,
          "function_name": "rl_for_reasoning_perception",
          "code": "text(\"**image**\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**image**",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 114,
          "function_name": "capability_perspective",
          "code": "rl_for_reasoning_perception()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 127,
          "function_name": "rl_for_reasoning_perception",
          "code": "text(\"- DeepEyes\"), link(arxiv_reference(\"https://arxiv.org/abs/2505.14362\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- DeepEyes",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "DeepEyes: Incentivizing \"Thinking with Images\" via Reinforcement Learning",
            "authors": [
              "Ziwei Zheng",
              "Michael Yang",
              "Jack Hong",
              "Chenxiao Zhao",
              "Guohai Xu",
              "Le Yang",
              "Chao Shen",
              "Xing Yu"
            ],
            "organization": null,
            "date": "2025-05-20T13:48:11Z",
            "url": "https://arxiv.org/abs/2505.14362",
            "description": "Large Vision-Language Models (VLMs) have shown strong capabilities in multimodal understanding and reasoning, yet they are primarily constrained by text-based reasoning processes. However, achieving seamless integration of visual and textual reasoning which mirrors human cognitive processes remains a significant challenge. In particular, effectively incorporating advanced visual input processing into reasoning mechanisms is still an open question. Thus, in this paper, we explore the interleaved multimodal reasoning paradigm and introduce DeepEyes, a model with \"thinking with images\" capabilities incentivized through end-to-end reinforcement learning without the need for cold-start SFT. Notably, this ability emerges natively within the model itself, leveraging its inherent grounding ability as a tool instead of depending on separate specialized models. Specifically, we propose a tool-use-oriented data selection mechanism and a reward strategy to encourage successful tool-assisted reasoning trajectories. DeepEyes achieves significant performance gains on fine-grained perception and reasoning benchmarks and also demonstrates improvement in grounding, hallucination, and mathematical reasoning tasks. Interestingly, we observe the distinct evolution of tool-calling behavior from initial exploration to efficient and accurate exploitation, and diverse thinking patterns that closely mirror human visual reasoning processes. Code is available at https://github.com/Visual-Agent/DeepEyes.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 114,
          "function_name": "capability_perspective",
          "code": "rl_for_reasoning_perception()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 128,
          "function_name": "rl_for_reasoning_perception",
          "code": "text(\"**video**\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**video**",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 114,
          "function_name": "capability_perspective",
          "code": "rl_for_reasoning_perception()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 129,
          "function_name": "rl_for_reasoning_perception",
          "code": "text(\"- Video-R1\"), link(arxiv_reference(\"https://arxiv.org/abs/2503.21776\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Video-R1",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
            "authors": [
              "Kaituo Feng",
              "Kaixiong Gong",
              "Bohao Li",
              "Zonghao Guo",
              "Yibing Wang",
              "Tianshuo Peng",
              "Junfei Wu",
              "Xiaoying Zhang",
              "Benyou Wang",
              "Xiangyu Yue"
            ],
            "organization": null,
            "date": "2025-03-27T17:59:51Z",
            "url": "https://arxiv.org/abs/2503.21776",
            "description": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for incentivizing video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-CoT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All code, models, and data are released in: https://github.com/tulerfeng/Video-R1.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 114,
          "function_name": "capability_perspective",
          "code": "rl_for_reasoning_perception()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 130,
          "function_name": "rl_for_reasoning_perception",
          "code": "text(\"**audio**\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**audio**",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 114,
          "function_name": "capability_perspective",
          "code": "rl_for_reasoning_perception()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 131,
          "function_name": "rl_for_reasoning_perception",
          "code": "text(\"- EchoInk-R1\"), link(arxiv_reference(\"https://arxiv.org/abs/2505.04623\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- EchoInk-R1",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning",
            "authors": [
              "Zhenghao Xing",
              "Xiaowei Hu",
              "Chi-Wing Fu",
              "Wenhai Wang",
              "Jifeng Dai",
              "Pheng-Ann Heng"
            ],
            "organization": null,
            "date": "2025-05-07T17:59:49Z",
            "url": "https://arxiv.org/abs/2505.04623",
            "description": "Multimodal large language models (MLLMs) have advanced perception across text, vision, and audio, yet they often struggle with structured cross-modal reasoning, particularly when integrating audio and visual signals. We introduce EchoInk-R1, a reinforcement learning framework that enhances such reasoning in MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice question answering over synchronized audio-image pairs. To enable this, we curate AVQA-R1-6K, a dataset pairing such audio-image inputs with multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves 85.77% accuracy on the validation set, outperforming the base model, which scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy, EchoInk-R1 demonstrates reflective reasoning by revisiting initial interpretations and refining responses when facing ambiguous multimodal inputs. These results suggest that lightweight reinforcement learning fine-tuning enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to unify audio, visual, and textual modalities for general open-world reasoning via reinforcement learning. Code and data are publicly released to facilitate further research.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 114,
          "function_name": "capability_perspective",
          "code": "rl_for_reasoning_perception()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 115,
          "function_name": "capability_perspective",
          "code": "rl_for_planning()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 115,
          "function_name": "capability_perspective",
          "code": "rl_for_planning()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 134,
          "function_name": "rl_for_planning",
          "code": "def rl_for_planning():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 115,
          "function_name": "capability_perspective",
          "code": "rl_for_planning()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 135,
          "function_name": "rl_for_planning",
          "code": "text(\"### RL as external driver\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### RL as external driver",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 115,
          "function_name": "capability_perspective",
          "code": "rl_for_planning()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 136,
          "function_name": "rl_for_planning",
          "code": "text(\"Reasoning with Language Model is Planning with World Model\"), link(arxiv_reference(\"https://arxiv.org/abs/2305.14992\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Reasoning with Language Model is Planning with World Model",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Reasoning with Language Model is Planning with World Model",
            "authors": [
              "Shibo Hao",
              "Yi Gu",
              "Haodi Ma",
              "Joshua Jiahua Hong",
              "Zhen Wang",
              "Daisy Zhe Wang",
              "Zhiting Hu"
            ],
            "organization": null,
            "date": "2023-05-24T10:28:28Z",
            "url": "https://arxiv.org/abs/2305.14992",
            "description": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 115,
          "function_name": "capability_perspective",
          "code": "rl_for_planning()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 137,
          "function_name": "rl_for_planning",
          "code": "image(\"images/agentic/rl_as_external_guide.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/rl_as_external_guide.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 115,
          "function_name": "capability_perspective",
          "code": "rl_for_planning()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 138,
          "function_name": "rl_for_planning",
          "code": "link(title=\"MCT Demo\", url=\"https://github.com/maitrix-org/llm-reasoners/blob/main/demo.ipynb\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "MCT Demo",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://github.com/maitrix-org/llm-reasoners/blob/main/demo.ipynb",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 115,
          "function_name": "capability_perspective",
          "code": "rl_for_planning()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 139,
          "function_name": "rl_for_planning",
          "code": "text(\"### RL as internal driver\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### RL as internal driver",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 115,
          "function_name": "capability_perspective",
          "code": "rl_for_planning()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 140,
          "function_name": "rl_for_planning",
          "code": "text(\"Encouraging Good Processes Without the Need for Good Answers\"), link(arxiv_reference(\"https://arxiv.org/abs/2508.19598\", organization=\"Tencent\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Encouraging Good Processes Without the Need for Good Answers",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning",
            "authors": [
              "Zhiwei Li",
              "Yong Hu",
              "Wenqing Wang"
            ],
            "organization": "Tencent",
            "date": "2025-08-27T06:19:50Z",
            "url": "https://arxiv.org/abs/2508.19598",
            "description": "The functionality of Large Language Model (LLM) agents is primarily determined by two capabilities: action planning and answer summarization. The former, action planning, is the core capability that dictates an agent's performance. However, prevailing training paradigms employ end-to-end, multi-objective optimization that jointly trains both capabilities. This paradigm faces two critical challenges: imbalanced optimization objective allocation and scarcity of verifiable data, making it difficult to enhance the agent's planning capability. To address these challenges, we propose Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that decouples the training process to enable a focused, single-objective optimization of the planning module. Crucially, RLTR introduces a reward signal based on tool-use completeness to directly evaluate the quality of tool invocation sequences. This method offers a more direct and reliable training signal than assessing the final response content, thereby obviating the need for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12% improvement in planning performance compared to end-to-end baselines. Moreover, this enhanced planning capability, in turn, translates to a 5%-6% increase in the final response quality of the overall agent system.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 115,
          "function_name": "capability_perspective",
          "code": "rl_for_planning()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 141,
          "function_name": "rl_for_planning",
          "code": "image(\"images/agentic/rl_as_internal_guide.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/rl_as_internal_guide.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 115,
          "function_name": "capability_perspective",
          "code": "rl_for_planning()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 142,
          "function_name": "rl_for_planning",
          "code": "text(r\"$$R_{total} = \\begin{cases}-1, & \\text{if trajectory format is invalid}, \\\\ R_{comp} + R_{rule}, & \\text{otherwise}. \\end{cases}$$\", rendering_type=\"mathjax\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "mathjax",
          "data": "$$R_{total} = \\begin{cases}-1, & \\text{if trajectory format is invalid}, \\\\ R_{comp} + R_{rule}, & \\text{otherwise}. \\end{cases}$$",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 115,
          "function_name": "capability_perspective",
          "code": "rl_for_planning()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 143,
          "function_name": "rl_for_planning",
          "code": "text(\"where $R_{comp}$ is the tooluse completeness reward, $R_{rule}$ is the rule based reward including a negative repetition reward Rrepeat to discourage redundant tool calls, and a negative reward Rerror as a penalty for incorrect tool usage. \")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "where $R_{comp}$ is the tooluse completeness reward, $R_{rule}$ is the rule based reward including a negative repetition reward Rrepeat to discourage redundant tool calls, and a negative reward Rerror as a penalty for incorrect tool usage. ",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 115,
          "function_name": "capability_perspective",
          "code": "rl_for_planning()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 146,
          "function_name": "rl_for_tool_use",
          "code": "def rl_for_tool_use():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 147,
          "function_name": "rl_for_tool_use",
          "code": "image(\"images/agentic/agent_tool_use.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/agent_tool_use.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 149,
          "function_name": "rl_for_tool_use",
          "code": "text(\"### ReAct-style Tool Calling\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### ReAct-style Tool Calling",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 150,
          "function_name": "rl_for_tool_use",
          "code": "text(\"Leverages 1)Prompt Engineering or 2)SFT-based methods to enable LLMs to follow the 'Observation-Thinking-Action' tool-use behaviors.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Leverages 1)Prompt Engineering or 2)SFT-based methods to enable LLMs to follow the 'Observation-Thinking-Action' tool-use behaviors.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 151,
          "function_name": "rl_for_tool_use",
          "code": "text(\"AgentBank\"), link(arxiv_reference(\"https://arxiv.org/abs/2410.07706\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "AgentBank",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories",
            "authors": [
              "Yifan Song",
              "Weimin Xiong",
              "Xiutian Zhao",
              "Dawei Zhu",
              "Wenhao Wu",
              "Ke Wang",
              "Cheng Li",
              "Wei Peng",
              "Sujian Li"
            ],
            "organization": null,
            "date": "2024-10-10T08:19:12Z",
            "url": "https://arxiv.org/abs/2410.07706",
            "description": "Fine-tuning on agent-environment interaction trajectory data holds significant promise for surfacing generalized agent capabilities in open-source large language models (LLMs). In this work, we introduce AgentBank, by far the largest trajectory tuning data collection featuring more than 50k diverse high-quality interaction trajectories which comprises 16 tasks covering five distinct agent skill dimensions. Leveraging a novel annotation pipeline, we are able to scale the annotated trajectories and generate a trajectory dataset with minimized difficulty bias. Furthermore, we fine-tune LLMs on AgentBank to get a series of agent models, Samoyed. Our comparative experiments demonstrate the effectiveness of scaling the interaction trajectory data to acquire generalized agent capabilities. Additional studies also reveal some key observations regarding trajectory tuning and agent skill generalization.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 152,
          "function_name": "rl_for_tool_use",
          "code": "image(\"images/agentic/agent_bank.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/agent_bank.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 154,
          "function_name": "rl_for_tool_use",
          "code": "text(\"### Tool-integrated RL\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Tool-integrated RL",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 155,
          "function_name": "rl_for_tool_use",
          "code": "text(\"Enables agents to strategically decide when, how, and in what combination to invoke tools\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Enables agents to strategically decide when, how, and in what combination to invoke tools",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 156,
          "function_name": "rl_for_tool_use",
          "code": "text(\"OpenAI o3 o4 DeepResearch&nbsp;\"), link(title=\"Introducing deep research\", url=\"https://openai.com/index/introducing-deep-research/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "OpenAI o3 o4 DeepResearch&nbsp;",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Introducing deep research",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://openai.com/index/introducing-deep-research/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 157,
          "function_name": "rl_for_tool_use",
          "code": "text(\"UI-TARS-2\"), link(arxiv_reference(\"https://arxiv.org/abs/2509.02544\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "UI-TARS-2",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
            "authors": [
              "Haoming Wang",
              "Haoyang Zou",
              "Huatong Song",
              "Jiazhan Feng",
              "Junjie Fang",
              "Junting Lu",
              "Longxiang Liu",
              "Qinyu Luo",
              "Shihao Liang",
              "Shijue Huang",
              "Wanjun Zhong",
              "Yining Ye",
              "Yujia Qin",
              "Yuwen Xiong",
              "Yuxin Song",
              "Zhiyong Wu",
              "Aoyan Li",
              "Bo Li",
              "Chen Dun",
              "Chong Liu",
              "Daoguang Zan",
              "Fuxing Leng",
              "Hanbin Wang",
              "Hao Yu",
              "Haobin Chen",
              "Hongyi Guo",
              "Jing Su",
              "Jingjia Huang",
              "Kai Shen",
              "Kaiyu Shi",
              "Lin Yan",
              "Peiyao Zhao",
              "Pengfei Liu",
              "Qinghao Ye",
              "Renjie Zheng",
              "Shulin Xin",
              "Wayne Xin Zhao",
              "Wen Heng",
              "Wenhao Huang",
              "Wenqian Wang",
              "Xiaobo Qin",
              "Yi Lin",
              "Youbin Wu",
              "Zehui Chen",
              "Zihao Wang",
              "Baoquan Zhong",
              "Xinchun Zhang",
              "Xujing Li",
              "Yuanfan Li",
              "Zhongkai Zhao",
              "Chengquan Jiang",
              "Faming Wu",
              "Haotian Zhou",
              "Jinlin Pang",
              "Li Han",
              "Qi Liu",
              "Qianli Ma",
              "Siyao Liu",
              "Songhua Cai",
              "Wenqi Fu",
              "Xin Liu",
              "Yaohui Wang",
              "Zhi Zhang",
              "Bo Zhou",
              "Guoliang Li",
              "Jiajun Shi",
              "Jiale Yang",
              "Jie Tang",
              "Li Li",
              "Qihua Han",
              "Taoran Lu",
              "Woyu Lin",
              "Xiaokang Tong",
              "Xinyao Li",
              "Yichi Zhang",
              "Yu Miao",
              "Zhengxuan Jiang",
              "Zili Li",
              "Ziyuan Zhao",
              "Chenxin Li",
              "Dehua Ma",
              "Feng Lin",
              "Ge Zhang",
              "Haihua Yang",
              "Hangyu Guo",
              "Hongda Zhu",
              "Jiaheng Liu",
              "Junda Du",
              "Kai Cai",
              "Kuanye Li",
              "Lichen Yuan",
              "Meilan Han",
              "Minchao Wang",
              "Shuyue Guo",
              "Tianhao Cheng",
              "Xiaobo Ma",
              "Xiaojun Xiao",
              "Xiaolong Huang",
              "Xinjie Chen",
              "Yidi Du",
              "Yilin Chen",
              "Yiwen Wang",
              "Zhaojian Li",
              "Zhenzhu Yang",
              "Zhiyuan Zeng",
              "Chaolin Jin",
              "Chen Li",
              "Hao Chen",
              "Haoli Chen",
              "Jian Chen",
              "Qinghao Zhao",
              "Guang Shi"
            ],
            "organization": null,
            "date": "2025-09-02T17:44:45Z",
            "url": "https://arxiv.org/abs/2509.02544",
            "description": "The development of autonomous agents for graphical user interfaces (GUIs) presents major challenges in artificial intelligence. While recent advances in native agent models have shown promise by unifying perception, reasoning, action, and memory through end-to-end learning, open problems remain in data scalability, multi-turn reinforcement learning (RL), the limitations of GUI-only operation, and environment stability. In this technical report, we present UI-TARS-2, a native GUI-centered agent model that addresses these challenges through a systematic training methodology: a data flywheel for scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI environment that integrates file systems and terminals, and a unified sandbox platform for large-scale rollouts. Empirical evaluation demonstrates that UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5. On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines such as Claude and OpenAI agents. In game environments, it attains a mean normalized score of 59.8 across a 15-game suite-roughly 60% of human-level performance-and remains competitive with frontier proprietary models (e.g., OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to long-horizon information-seeking tasks and software engineering benchmarks, highlighting its robustness across diverse agent tasks. Detailed analyses of training dynamics further provide insights into achieving stability and efficiency in large-scale agent RL. These results underscore UI-TARS-2's potential to advance the state of GUI agents and exhibit strong generalization to real-world interactive scenarios.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 158,
          "function_name": "rl_for_tool_use",
          "code": "link(title=\"UI-TARS-2 DEMO\", url=\"https://seed-tars.com/showcase/ui-tars-2/\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "UI-TARS-2 DEMO",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://seed-tars.com/showcase/ui-tars-2/",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 159,
          "function_name": "rl_for_tool_use",
          "code": "image(\"images/agentic/UI-TARS-2.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/UI-TARS-2.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 161,
          "function_name": "rl_for_tool_use",
          "code": "text(\"### Long-horizon RL\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### Long-horizon RL",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 162,
          "function_name": "rl_for_tool_use",
          "code": "text(\"Current RL approaches often depend on outcome-based rewards, making it difficult to pinpoint which specific tool invocation in a long, interdependent sequence contributed to success or failure.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Current RL approaches often depend on outcome-based rewards, making it difficult to pinpoint which specific tool invocation in a long, interdependent sequence contributed to success or failure.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 163,
          "function_name": "rl_for_tool_use",
          "code": "text(\"step-level advantage estimation in SpaRL\"), link(arxiv_reference(\"https://arxiv.org/abs/2505.20732\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "step-level advantage estimation in SpaRL",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution",
            "authors": [
              "Hanlin Wang",
              "Chak Tou Leong",
              "Jiashuo Wang",
              "Jian Wang",
              "Wenjie Li"
            ],
            "organization": null,
            "date": "2025-05-27T05:21:04Z",
            "url": "https://arxiv.org/abs/2505.20732",
            "description": "Reinforcement learning (RL) holds significant promise for training LLM agents to handle complex, goal-oriented tasks that require multi-step interactions with external environments. However, a critical challenge when applying RL to these agentic tasks arises from delayed rewards: feedback signals are typically available only after the entire task is completed. This makes it non-trivial to assign delayed rewards to earlier actions, providing insufficient guidance regarding environmental constraints and hindering agent training. In this work, we draw on the insight that the ultimate completion of a task emerges from the cumulative progress an agent makes across individual steps. We propose Stepwise Progress Attribution (SPA), a general reward redistribution framework that decomposes the final reward into stepwise contributions, each reflecting its incremental progress toward overall task completion. To achieve this, we train a progress estimator that accumulates stepwise contributions over a trajectory to match the task completion. During policy optimization, we combine the estimated per-step contribution with a grounding signal for actions executed in the environment as the fine-grained, intermediate reward for effective agent training. Extensive experiments on common agent benchmarks (including Webshop, ALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the state-of-the-art method in both success rate (+2.5\\% on average) and grounding accuracy (+1.9\\% on average). Further analyses demonstrate that our method remarkably provides more effective intermediate rewards for RL training. Our code is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 164,
          "function_name": "rl_for_tool_use",
          "code": "image(\"images/agentic/SpaRL.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/SpaRL.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 116,
          "function_name": "capability_perspective",
          "code": "rl_for_tool_use()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 167,
          "function_name": "rl_for_agent_memory",
          "code": "def rl_for_agent_memory():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 168,
          "function_name": "rl_for_agent_memory",
          "code": "image(\"images/agentic/rl_for_memory_overview.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/rl_for_memory_overview.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 169,
          "function_name": "rl_for_agent_memory",
          "code": "text(\"### RAG-style\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### RAG-style",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 170,
          "function_name": "rl_for_agent_memory",
          "code": "text(\"MemoryBank\"), link(arxiv_reference(\"https://arxiv.org/abs/2305.10250\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "MemoryBank",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
            "authors": [
              "Wanjun Zhong",
              "Lianghong Guo",
              "Qiqi Gao",
              "He Ye",
              "Yanlin Wang"
            ],
            "organization": null,
            "date": "2023-05-17T14:40:29Z",
            "url": "https://arxiv.org/abs/2305.10250",
            "description": "Revolutionary advancements in Large Language Models have drastically reshaped our interactions with artificial intelligence systems. Despite this, a notable hindrance remains-the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems and psychological counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user personality by synthesizing information from past interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory, which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a human-like memory mechanism. MemoryBank is versatile in accommodating both closed-source models like ChatGPT and open-source models like ChatGLM. We exemplify application of MemoryBank through the creation of an LLM-based chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned with psychological dialogs, SiliconFriend displays heightened empathy in its interactions. Experiment involves both qualitative analysis with real-world user dialogs and quantitative analysis with simulated dialogs. In the latter, ChatGPT acts as users with diverse characteristics and generates long-term dialog contexts covering a wide array of topics. The results of our analysis reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong capability for long-term companionship as it can provide emphatic response, recall relevant memories and understand user personality.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 171,
          "function_name": "rl_for_agent_memory",
          "code": "image(\"images/agentic/memory_bank.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/memory_bank.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 172,
          "function_name": "rl_for_agent_memory",
          "code": "text(\"Memory-R1\"), link(arxiv_reference(\"https://arxiv.org/abs/2508.19828\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Memory-R1",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning",
            "authors": [
              "Sikuan Yan",
              "Xiufeng Yang",
              "Zuchao Huang",
              "Ercong Nie",
              "Zifeng Ding",
              "Zonggen Li",
              "Xiaowen Ma",
              "Hinrich Sch\u00fctze",
              "Volker Tresp",
              "Yunpu Ma"
            ],
            "organization": null,
            "date": "2025-08-27T12:26:55Z",
            "url": "https://arxiv.org/abs/2508.19828",
            "description": "Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of NLP tasks, but they remain fundamentally stateless, constrained by limited context windows that hinder long-horizon reasoning. Recent efforts to address this limitation often augment LLMs with an external memory bank, yet most existing pipelines are static and heuristic-driven, lacking any learned mechanism for deciding what to store, update, or retrieve. We present Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the ability to actively manage and utilize external memory through two specialized agents: a Memory Manager that learns to perform structured memory operations, including adding, updating, deleting, or taking no operation on memory entries; and an Answer Agent that selects the most relevant entries and reasons over them to produce an answer. Both agents are fine-tuned with outcome-driven RL (PPO and GRPO), enabling adaptive memory management and utilization with minimal supervision. With as few as 152 question-answer pairs and a corresponding temporal memory bank for training, Memory-R1 outperforms the strongest existing baseline and demonstrates strong generalization across diverse question types and LLM backbones. Beyond presenting an effective approach, this work provides insights into how RL can unlock more agentic, memory-aware behavior in LLMs, pointing toward richer, more persistent reasoning systems.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 173,
          "function_name": "rl_for_agent_memory",
          "code": "image(\"images/agentic/memory-r1.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/memory-r1.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 174,
          "function_name": "rl_for_agent_memory",
          "code": "image(\"images/agentic/overview-of-memory-r1.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/overview-of-memory-r1.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 176,
          "function_name": "rl_for_agent_memory",
          "code": "text(\"### RL for Token-level Memory\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### RL for Token-level Memory",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 177,
          "function_name": "rl_for_agent_memory",
          "code": "text(\"MemAgent\"), link(arxiv_reference(\"https://arxiv.org/abs/2507.02259\", organization=\"ByteDance\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "MemAgent",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent",
            "authors": [
              "Hongli Yu",
              "Tinghong Chen",
              "Jiangtao Feng",
              "Jiangjie Chen",
              "Weinan Dai",
              "Qiying Yu",
              "Ya-Qin Zhang",
              "Wei-Ying Ma",
              "Jingjing Liu",
              "Mingxuan Wang",
              "Hao Zhou"
            ],
            "organization": "ByteDance",
            "date": "2025-07-03T03:11:50Z",
            "url": "https://arxiv.org/abs/2507.02259",
            "description": "Despite improvements by length extrapolation, efficient attention and memory modules, handling infinitely long documents with linear complexity without performance degradation during extrapolation remains the ultimate challenge in long-text processing. We directly optimize for long-text tasks in an end-to-end fashion and introduce a novel agent workflow, MemAgent, which reads text in segments and updates the memory using an overwrite strategy. We extend the DAPO algorithm to facilitate training via independent-context multi-conversation generation. MemAgent has demonstrated superb long-context capabilities, being able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task with performance loss < 5% and achieves 95%+ in 512K RULER test.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 178,
          "function_name": "rl_for_agent_memory",
          "code": "image(\"images/agentic/mem_agent.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/mem_agent.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 179,
          "function_name": "rl_for_agent_memory",
          "code": "text(\"ReSum\"), link(arxiv_reference(\"https://arxiv.org/abs/2509.13313\", organization=\"Alibaba\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "ReSum",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
            "authors": [
              "Xixi Wu",
              "Kuan Li",
              "Yida Zhao",
              "Liwen Zhang",
              "Litu Ou",
              "Huifeng Yin",
              "Zhongwang Zhang",
              "Yong Jiang",
              "Pengjun Xie",
              "Fei Huang",
              "Minhao Cheng",
              "Shuai Wang",
              "Hong Cheng",
              "Jingren Zhou"
            ],
            "organization": "Alibaba",
            "date": "2025-09-16T17:57:22Z",
            "url": "https://arxiv.org/abs/2509.13313",
            "description": "Large Language Model (LLM)-based web agents demonstrate strong performance on knowledge-intensive tasks but are hindered by context window limitations in paradigms like ReAct. Complex queries involving multiple entities, intertwined relationships, and high uncertainty demand extensive search cycles that rapidly exhaust context budgets before reaching complete solutions. To overcome this challenge, we introduce ReSum, a novel paradigm that enables indefinite exploration through periodic context summarization. ReSum converts growing interaction histories into compact reasoning states, maintaining awareness of prior discoveries while bypassing context constraints. For paradigm adaptation, we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and advantage broadcasting to familiarize agents with summary-conditioned reasoning. Extensive experiments on web agents of varying scales across three benchmarks demonstrate that ReSum delivers an average absolute improvement of 4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO training. Notably, with only 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on BrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web agents.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 180,
          "function_name": "rl_for_agent_memory",
          "code": "image(\"images/agentic/resum.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/resum.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 181,
          "function_name": "rl_for_agent_memory",
          "code": "text(\"**example for qwen deepresearch:**\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**example for qwen deepresearch:**",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 182,
          "function_name": "rl_for_agent_memory",
          "code": "image(\"images/agentic/qwen_deep_research.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/qwen_deep_research.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 183,
          "function_name": "rl_for_agent_memory",
          "code": "link(title=\"\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08Agentic RL\uff09\u6df1\u5ea6\u7814\u7a76\u62a5\u544a.pdf\", url=\"var/files/\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08Agentic RL\uff09\u6df1\u5ea6\u7814\u7a76\u62a5\u544a.pdf\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08Agentic RL\uff09\u6df1\u5ea6\u7814\u7a76\u62a5\u544a.pdf",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "var/files/\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08Agentic RL\uff09\u6df1\u5ea6\u7814\u7a76\u62a5\u544a.pdf",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 117,
          "function_name": "capability_perspective",
          "code": "rl_for_agent_memory()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 186,
          "function_name": "rl_for_self_improvement",
          "code": "def rl_for_self_improvement():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 187,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"- RL for Verbal Self-correction.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- RL for Verbal Self-correction.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 188,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"- RL for Internalizing Self-correction.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- RL for Internalizing Self-correction.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 189,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"- RL for Iterative Self-training.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- RL for Iterative Self-training.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 190,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"---\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "---",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 191,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"### RL for Verbal Self-correction\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### RL for Verbal Self-correction",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 192,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"Utilizing prompt engineering/rl base methods.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Utilizing prompt engineering/rl base methods.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 193,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"Agents generate an answer, linguistically reflect on its potential errors, and subsequently produce a refined solution.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Agents generate an answer, linguistically reflect on its potential errors, and subsequently produce a refined solution.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 194,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"### RL for Internalizing Self-correction\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### RL for Internalizing Self-correction",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 195,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"ACC-Collab\"), link(arxiv_reference(\"https://arxiv.org/abs/2411.00053\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "ACC-Collab",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration",
            "authors": [
              "Andrew Estornell",
              "Jean-Francois Ton",
              "Yuanshun Yao",
              "Yang Liu"
            ],
            "organization": null,
            "date": "2024-10-30T19:09:02Z",
            "url": "https://arxiv.org/abs/2411.00053",
            "description": "Large language models (LLMs) have demonstrated a remarkable ability to serve as general-purpose tools for various language-based tasks. Recent works have demonstrated that the efficacy of such models can be improved through iterative dialog between multiple models. While these paradigms show promise in improving model efficacy, most works in this area treat collaboration as an emergent behavior, rather than a learned behavior. In doing so, current multi-agent frameworks rely on collaborative behaviors to have been sufficiently trained into off-the-shelf models. To address this limitation, we propose ACC-Collab, an Actor-Critic based learning framework to produce a two-agent team (an actor-agent and a critic-agent) specialized in collaboration. We demonstrate that ACC-Collab outperforms SotA multi-agent techniques on a wide array of benchmarks.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 196,
          "function_name": "rl_for_self_improvement",
          "code": "image(\"images/agentic/actor_critic.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/actor_critic.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 197,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"### RL for Iterative Self-training\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### RL for Iterative Self-training",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 198,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"Absolute Zero\"), link(arxiv_reference(\"https://arxiv.org/abs/2505.03335\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Absolute Zero",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "authors": [
              "Andrew Zhao",
              "Yiran Wu",
              "Yang Yue",
              "Tong Wu",
              "Quentin Xu",
              "Yang Yue",
              "Matthieu Lin",
              "Shenzhi Wang",
              "Qingyun Wu",
              "Zilong Zheng",
              "Gao Huang"
            ],
            "organization": null,
            "date": "2025-05-06T09:08:00Z",
            "url": "https://arxiv.org/abs/2505.03335",
            "description": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 199,
          "function_name": "rl_for_self_improvement",
          "code": "image(\"images/agentic/absolute_zero.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/absolute_zero.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 200,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"Learning Different Modes of Reasoning:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Learning Different Modes of Reasoning:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 201,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"Deduction(\u6f14\u7ece): predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Deduction(\u6f14\u7ece): predicting the output o given a program p and input i, capturing step-by-step logical reasoning.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 202,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"Abduction(\u6eaf\u56e0): inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Abduction(\u6eaf\u56e0): inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 203,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"Induction(\u5f52\u7eb3): synthesizing a program p from a set of in-out examples, requiring generalization from partial information\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Induction(\u5f52\u7eb3): synthesizing a program p from a set of in-out examples, requiring generalization from partial information",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 205,
          "function_name": "rl_for_self_improvement",
          "code": "text(\"MAPoRL\"), link(arxiv_reference(\"https://arxiv.org/abs/2502.18439\"))"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "MAPoRL",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning",
            "authors": [
              "Chanwoo Park",
              "Seungju Han",
              "Xingzhi Guo",
              "Asuman Ozdaglar",
              "Kaiqing Zhang",
              "Joo-Kyung Kim"
            ],
            "organization": null,
            "date": "2025-02-25T18:33:48Z",
            "url": "https://arxiv.org/abs/2502.18439",
            "description": "Leveraging multiple large language models (LLMs) to build collaborative multi-agentic workflows has demonstrated significant potential. However, most previous studies focus on prompting the out-of-the-box LLMs, relying on their innate capability for collaboration, which may not improve LLMs' performance as shown recently. In this paper, we introduce a new post-training paradigm MAPoRL (Multi-Agent Post-co-training for collaborative LLMs with Reinforcement Learning), to explicitly elicit the collaborative behaviors and further unleash the power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first generate their own responses independently and engage in a multi-turn discussion to collaboratively improve the final answer. In the end, a MAPoRL verifier evaluates both the answer and the discussion, by assigning a score that verifies the correctness of the answer, while adding incentives to encourage corrective and persuasive discussions. The score serves as the co-training reward, and is then maximized through multi-agent RL. Unlike existing LLM post-training paradigms, MAPoRL advocates the co-training of multiple LLMs together using RL for better generalization. Accompanied by analytical insights, our experiments demonstrate that training individual LLMs alone is insufficient to induce effective collaboration. In contrast, multi-agent co-training can boost the collaboration performance across benchmarks, with generalization to unseen domains.",
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 206,
          "function_name": "rl_for_self_improvement",
          "code": "image(\"images/agentic/maporl.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/maporl.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 207,
          "function_name": "rl_for_self_improvement",
          "code": "image(\"images/agentic/maporl_demo.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/maporl_demo.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 118,
          "function_name": "capability_perspective",
          "code": "rl_for_self_improvement()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 13,
          "function_name": "main",
          "code": "capability_perspective()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 14,
          "function_name": "main",
          "code": "vertical_domains()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 14,
          "function_name": "main",
          "code": "vertical_domains()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 210,
          "function_name": "vertical_domains",
          "code": "def vertical_domains():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 14,
          "function_name": "main",
          "code": "vertical_domains()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 211,
          "function_name": "vertical_domains",
          "code": "image(\"images/agentic/vertical_domains.jpg\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/vertical_domains.jpg",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 14,
          "function_name": "main",
          "code": "vertical_domains()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 16,
          "function_name": "main",
          "code": "link(title=\"A Tool Demo Based on Qwen3\", url=\"https://donpromax.github.io/trace-viewer/?trace=var%2Ftraces%2Ftool_call_example.json\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "A Tool Demo Based on Qwen3",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://donpromax.github.io/trace-viewer/?trace=var%2Ftraces%2Ftool_call_example.json",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 18,
          "function_name": "main",
          "code": "rstar2_agent()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 18,
          "function_name": "main",
          "code": "rstar2_agent()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 214,
          "function_name": "rstar2_agent",
          "code": "def rstar2_agent():"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 18,
          "function_name": "main",
          "code": "rstar2_agent()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 215,
          "function_name": "rstar2_agent",
          "code": "text(\"rStar2-Agent: Agentic Reasoning Technical Report\"), link(rstar_agent)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "rStar2-Agent: Agentic Reasoning Technical Report",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "rStar2-Agent: Agentic Reasoning Technical Report",
            "authors": [
              "Ning Shang",
              "Yifei Liu",
              "Yi Zhu",
              "Li Lyna Zhang",
              "Weijiang Xu",
              "Xinyu Guan",
              "Buze Zhang",
              "Bingcheng Dong",
              "Xudong Zhou",
              "Bowen Zhang",
              "Ying Xin",
              "Ziming Miao",
              "Scarlett Li",
              "Fan Yang",
              "Mao Yang"
            ],
            "organization": "Microsoft",
            "date": "2025-08-28T12:45:25Z",
            "url": "https://arxiv.org/pdf/2508.20722",
            "description": "We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar.",
            "notes": "Python tool calling, code interpreter as environment"
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 18,
          "function_name": "main",
          "code": "rstar2_agent()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 216,
          "function_name": "rstar2_agent",
          "code": "text(\"## Overview\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "## Overview",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 18,
          "function_name": "main",
          "code": "rstar2_agent()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 217,
          "function_name": "rstar2_agent",
          "code": "text(\"Based on Qwen3-14B trained with agentic reinforcement learning to achieve frontier-level performance\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Based on Qwen3-14B trained with agentic reinforcement learning to achieve frontier-level performance",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 18,
          "function_name": "main",
          "code": "rstar2_agent()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 218,
          "function_name": "rstar2_agent",
          "code": "image(\"images/agentic/rstar2_bench.png\", width=600, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/rstar2_bench.png",
          "style": {
            "width": 600,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 18,
          "function_name": "main",
          "code": "rstar2_agent()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 219,
          "function_name": "rstar2_agent",
          "code": "text(\"### An example\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "### An example",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 18,
          "function_name": "main",
          "code": "rstar2_agent()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 220,
          "function_name": "rstar2_agent",
          "code": "text(\"Agent can use Python coding tools and reflect on code execution feedback\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Agent can use Python coding tools and reflect on code execution feedback",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 18,
          "function_name": "main",
          "code": "rstar2_agent()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 221,
          "function_name": "rstar2_agent",
          "code": "image(\"images/agentic/rstar2_demo.png\", width=800, center=True)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/agentic/rstar2_demo.png",
          "style": {
            "width": 800,
            "display": "block",
            "margin": "0 auto"
          },
          "external_link": null,
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 18,
          "function_name": "main",
          "code": "rstar2_agent()"
        },
        {
          "path": "agentic_rl.py",
          "line_number": 222,
          "function_name": "rstar2_agent",
          "code": "text(\"full information:\"), link(\"https://yuque.antfin.com/vihe9q/oeo4yp/dfunmnxdwq8gxbyn\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "full information:",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": null,
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://yuque.antfin.com/vihe9q/oeo4yp/dfunmnxdwq8gxbyn",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 18,
          "function_name": "main",
          "code": "rstar2_agent()"
        }
      ],
      "env": {},
      "renderings": [],
      "stdout": "",
      "stderr": ""
    },
    {
      "stack": [
        {
          "path": "agentic_rl.py",
          "line_number": 20,
          "function_name": "main",
          "code": "link(title=\"rStar2 Agent Demo\", url=\"https://donpromax.github.io/trace-viewer/?trace=var%2Ftraces%2Frstar_agent.json\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "rStar2 Agent Demo",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://donpromax.github.io/trace-viewer/?trace=var%2Ftraces%2Frstar_agent.json",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ],
      "stdout": "",
      "stderr": ""
    }
  ]
}
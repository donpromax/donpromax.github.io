<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%26id_list%3D2410.07706%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=&amp;id_list=2410.07706&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/83vIY/VfHU6qY57y7OY/v3r+Osw</id>
  <updated>2025-09-25T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2410.07706v1</id>
    <updated>2024-10-10T08:19:12Z</updated>
    <published>2024-10-10T08:19:12Z</published>
    <title>AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+
  Interaction Trajectories</title>
    <summary>  Fine-tuning on agent-environment interaction trajectory data holds
significant promise for surfacing generalized agent capabilities in open-source
large language models (LLMs). In this work, we introduce AgentBank, by far the
largest trajectory tuning data collection featuring more than 50k diverse
high-quality interaction trajectories which comprises 16 tasks covering five
distinct agent skill dimensions. Leveraging a novel annotation pipeline, we are
able to scale the annotated trajectories and generate a trajectory dataset with
minimized difficulty bias. Furthermore, we fine-tune LLMs on AgentBank to get a
series of agent models, Samoyed. Our comparative experiments demonstrate the
effectiveness of scaling the interaction trajectory data to acquire generalized
agent capabilities. Additional studies also reveal some key observations
regarding trajectory tuning and agent skill generalization.
</summary>
    <author>
      <name>Yifan Song</name>
    </author>
    <author>
      <name>Weimin Xiong</name>
    </author>
    <author>
      <name>Xiutian Zhao</name>
    </author>
    <author>
      <name>Dawei Zhu</name>
    </author>
    <author>
      <name>Wenhao Wu</name>
    </author>
    <author>
      <name>Ke Wang</name>
    </author>
    <author>
      <name>Cheng Li</name>
    </author>
    <author>
      <name>Wei Peng</name>
    </author>
    <author>
      <name>Sujian Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Findings of EMNLP 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.07706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.07706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>

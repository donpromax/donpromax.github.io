<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%26id_list%3D2503.21776%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=&amp;id_list=2503.21776&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/L0UBmXUmLsMX5PE6xJZwNmu/zO4</id>
  <updated>2025-09-25T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.21776v3</id>
    <updated>2025-05-15T07:28:30Z</updated>
    <published>2025-03-27T17:59:51Z</published>
    <title>Video-R1: Reinforcing Video Reasoning in MLLMs</title>
    <summary>  Inspired by DeepSeek-R1's success in eliciting reasoning abilities through
rule-based reinforcement learning (RL), we introduce Video-R1 as the first
attempt to systematically explore the R1 paradigm for incentivizing video
reasoning within multimodal large language models (MLLMs). However, directly
applying RL training with the GRPO algorithm to video reasoning presents two
primary challenges: (i) a lack of temporal modeling for video reasoning, and
(ii) the scarcity of high-quality video-reasoning data. To address these
issues, we first propose the T-GRPO algorithm, which encourages models to
utilize temporal information in videos for reasoning. Additionally, instead of
relying solely on video data, we incorporate high-quality image-reasoning data
into the training process. We have constructed two datasets: Video-R1-CoT-165k
for SFT cold start and Video-R1-260k for RL training, both comprising image and
video data. Experimental results demonstrate that Video-R1 achieves significant
improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as
well as on general video benchmarks including MVBench and TempCompass, etc.
Notably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoning
benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All
code, models, and data are released in: https://github.com/tulerfeng/Video-R1.
</summary>
    <author>
      <name>Kaituo Feng</name>
    </author>
    <author>
      <name>Kaixiong Gong</name>
    </author>
    <author>
      <name>Bohao Li</name>
    </author>
    <author>
      <name>Zonghao Guo</name>
    </author>
    <author>
      <name>Yibing Wang</name>
    </author>
    <author>
      <name>Tianshuo Peng</name>
    </author>
    <author>
      <name>Junfei Wu</name>
    </author>
    <author>
      <name>Xiaoying Zhang</name>
    </author>
    <author>
      <name>Benyou Wang</name>
    </author>
    <author>
      <name>Xiangyu Yue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://github.com/tulerfeng/Video-R1</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.21776v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.21776v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>

<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%26id_list%3D2502.18439%26start%3D0%26max_results%3D10" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=&amp;id_list=2502.18439&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/Be7CbCa8EaYegaAkXb0Na478hYQ</id>
  <updated>2025-09-25T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2502.18439v2</id>
    <updated>2025-07-12T20:13:27Z</updated>
    <published>2025-02-25T18:33:48Z</published>
    <title>MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language
  Models with Reinforcement Learning</title>
    <summary>  Leveraging multiple large language models (LLMs) to build collaborative
multi-agentic workflows has demonstrated significant potential. However, most
previous studies focus on prompting the out-of-the-box LLMs, relying on their
innate capability for collaboration, which may not improve LLMs' performance as
shown recently. In this paper, we introduce a new post-training paradigm MAPoRL
(Multi-Agent Post-co-training for collaborative LLMs with Reinforcement
Learning), to explicitly elicit the collaborative behaviors and further unleash
the power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first
generate their own responses independently and engage in a multi-turn
discussion to collaboratively improve the final answer. In the end, a MAPoRL
verifier evaluates both the answer and the discussion, by assigning a score
that verifies the correctness of the answer, while adding incentives to
encourage corrective and persuasive discussions. The score serves as the
co-training reward, and is then maximized through multi-agent RL. Unlike
existing LLM post-training paradigms, MAPoRL advocates the co-training of
multiple LLMs together using RL for better generalization. Accompanied by
analytical insights, our experiments demonstrate that training individual LLMs
alone is insufficient to induce effective collaboration. In contrast,
multi-agent co-training can boost the collaboration performance across
benchmarks, with generalization to unseen domains.
</summary>
    <author>
      <name>Chanwoo Park</name>
    </author>
    <author>
      <name>Seungju Han</name>
    </author>
    <author>
      <name>Xingzhi Guo</name>
    </author>
    <author>
      <name>Asuman Ozdaglar</name>
    </author>
    <author>
      <name>Kaiqing Zhang</name>
    </author>
    <author>
      <name>Joo-Kyung Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">version for ACL</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.18439v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.18439v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
